{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (DatabaseError('database disk image is malformed')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import spotipy\n",
    "from spotipy.oauth2 import SpotifyOAuth\n",
    "from spotipy.oauth2 import SpotifyClientCredentials\n",
    "import random\n",
    "import time\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Connect to Spotify API Developer Account\n",
    "sp = spotipy.Spotify(\n",
    "    client_credentials_manager=SpotifyClientCredentials(\n",
    "        client_id=\"f4499c1475cc48b3bb23c9e8215f37ac\",\n",
    "        client_secret=\"a2ced9ec9e3845d4b1be65d5e0f097e4\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to fetch metadata for multiple songs\n",
    "def fetch_songs_metadata(sp, prefixes, output_file, max_songs=30000):\n",
    "    collected_songs = []\n",
    "    try:\n",
    "        for prefix in prefixes:\n",
    "            for offset in range(0, 1000, 50):  # Pagination\n",
    "                if len(collected_songs) >= max_songs:\n",
    "                    break  # Stop collecting if max_songs is reached\n",
    "                try:\n",
    "                    results = sp.search(q=f\"{prefix} year:2022\", type=\"track\", limit=50, offset=offset)\n",
    "                    for track in results['tracks']['items']:\n",
    "                        # Extract required metadata\n",
    "                        song_data = {\n",
    "                            \"id\": track['id'],\n",
    "                            \"name\": track['name'],\n",
    "                            \"artist\": track['artists'][0]['name'],\n",
    "                            \"popularity\": track['popularity'],\n",
    "                            \"duration_ms\": track['duration_ms'],\n",
    "                            \"explicit\": track['explicit']\n",
    "                        }\n",
    "                        collected_songs.append(song_data)\n",
    "                        if len(collected_songs) >= max_songs:\n",
    "                            break  # Stop collecting if max_songs is reached\n",
    "\n",
    "                    if len(results['tracks']['items']) == 0:\n",
    "                        break  # No more results for this prefix\n",
    "\n",
    "                    # Delay to respect rate limits\n",
    "                    time.sleep(1)\n",
    "\n",
    "                except spotipy.exceptions.SpotifyException as e:\n",
    "                    if e.http_status == 429:  # Rate limit error\n",
    "                        retry_after = int(e.headers.get('Retry-After', 5))\n",
    "                        print(f\"Rate limited. Retrying after {retry_after} seconds...\")\n",
    "                        time.sleep(retry_after)\n",
    "                    else:\n",
    "                        print(f\"SpotifyException: {e}\")\n",
    "                        break\n",
    "\n",
    "        # Save collected data to a CSV file\n",
    "        pd.DataFrame(collected_songs).to_csv(output_file, index=False)\n",
    "        print(f\"Collected metadata for {len(collected_songs)} songs. Saved to {output_file}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Critical error during song collection: {e}\")\n",
    "\n",
    "# Example usage\n",
    "prefixes = [chr(i) for i in range(97, 123)]  # 'a' to 'z'\n",
    "output_file = \"songs_metadata.csv\"\n",
    "\n",
    "# Collect metadata for up to 100 songs\n",
    "fetch_songs_metadata(sp, prefixes, output_file, max_songs=30000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE): 134.04618002779034\n",
      "Test set MSE: 11.577831404360246\n"
     ]
    }
   ],
   "source": [
    "#Lasso Regression Unoptimized:\n",
    "\n",
    "# Assuming your dataset is saved in a CSV or DataFrame called 'df'\n",
    "df = pd.read_csv('songs_metadata.csv')\n",
    "\n",
    "# Independent and dependent variables\n",
    "X = df[['artist', 'duration_ms', 'explicit']]\n",
    "y = df['popularity']\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Preprocessing pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('artist_enc', OneHotEncoder(handle_unknown='ignore'), ['artist']),\n",
    "        ('scale', StandardScaler(), ['duration_ms']),\n",
    "        ('pass', 'passthrough', ['explicit'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Build pipeline with Lasso regression\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('lasso', Lasso(alpha=0.1))  # Adjust alpha as needed\n",
    "])\n",
    "\n",
    "# Fit the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "test_rmse = np.sqrt(mse)\n",
    "print(\"Mean Squared Error (MSE):\", mse)\n",
    "print(f\"Test set MSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best alpha: 0.001\n",
      "Best cross-validated MSE: 85.36714963684665\n",
      "Test set MSE: 80.72035179365008\n",
      "Test set MSE: 8.984450556024564\n"
     ]
    }
   ],
   "source": [
    "#Optimized Lasso Regression (Best Model)\n",
    "\n",
    "# Define the preprocessing pipeline (same as before)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('artist_enc', OneHotEncoder(handle_unknown='ignore'), ['artist']),\n",
    "        ('scale', StandardScaler(), ['duration_ms']),\n",
    "        ('pass', 'passthrough', ['explicit'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Lasso regression pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('lasso', Lasso())\n",
    "])\n",
    "\n",
    "# Define the grid of alpha values to search\n",
    "param_grid = {\n",
    "    'lasso__alpha': np.logspace(-3, 2, 20)  # Alphas ranging from 0.001 to 100\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring='neg_mean_squared_error',  # Use MSE as the scoring metric (negated)\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best alpha and corresponding MSE\n",
    "best_alpha = grid_search.best_params_['lasso__alpha']\n",
    "best_mse = -grid_search.best_score_\n",
    "\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(f\"Best cross-validated MSE: {best_mse}\")\n",
    "\n",
    "# Test set performance\n",
    "best_model = grid_search.best_estimator_\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "print(f\"Test set MSE: {test_mse}\")\n",
    "print(f\"Test set MSE: {test_rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error on the test set: 134.14224191516598\n",
      "RMSE on the test set: 11.581979188168402\n"
     ]
    }
   ],
   "source": [
    "#Standard Linear Regressor (No Regularization)\n",
    "\n",
    "# Features and target variable\n",
    "X = df[['artist', 'duration_ms', 'explicit']]\n",
    "y = df['popularity']\n",
    "\n",
    "# One-hot encode the 'artist' column and keep the other features\n",
    "# Define the transformer for categorical encoding\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('artist_encoder', OneHotEncoder(handle_unknown='ignore'), ['artist']),\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep the other numeric and boolean features\n",
    ")\n",
    "\n",
    "# Create the pipeline with preprocessing and regression model\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('regressor', LinearRegression())\n",
    "])\n",
    "\n",
    "# Split data into training and test sets (70/30 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Calculate the Mean Squared Error\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "print(f\"Mean Squared Error on the test set: {mse}\")\n",
    "print(f\"RMSE on the test set: {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 6 candidates, totalling 30 fits\n",
      "Optimal alpha: 1000\n",
      "Mean Squared Error (MSE): 134.14224191516453\n",
      "Root Mean Squared Error (RMSE): 11.581979188168338\n"
     ]
    }
   ],
   "source": [
    "#Linear Regressor with Optimized Ridge Regularization\n",
    "\n",
    "# Define the preprocessing for categorical and numerical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('artist_encoder', OneHotEncoder(handle_unknown='ignore'), ['artist']),\n",
    "    ],\n",
    "    remainder='passthrough'  # Keep the numerical and boolean features unchanged\n",
    ")\n",
    "\n",
    "# Define Ridge Regression pipeline\n",
    "ridge_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('ridge_regressor', Ridge())  # No alpha here; GridSearchCV will handle it\n",
    "])\n",
    "\n",
    "# Define a range of alpha values\n",
    "alpha_values = [0.01, 0.1, 1, 10, 100, 1000]\n",
    "param_grid = {'ridge_regressor__alpha': alpha_values}\n",
    "\n",
    "# Use GridSearchCV to find the best alpha\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=ridge_pipeline,\n",
    "    param_grid=param_grid,\n",
    "    scoring='neg_mean_squared_error',  # Use negative MSE\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the model\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best alpha\n",
    "best_alpha = grid_search.best_params_['ridge_regressor__alpha']\n",
    "print(f\"Optimal alpha: {best_alpha}\")\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "best_model = grid_search.best_estimator_\n",
    "y_pred = best_model.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Root Mean Squared Error (RMSE): {rmse}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best alpha: 0.42813323987193913\n",
      "Best cross-validated MSE: 77.9505026275579\n",
      "Test set MSE: 72.062473501812\n",
      "Test set RMSE: 8.488961862431236\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Define the preprocessing pipeline (same as before)\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('artist_enc', OneHotEncoder(handle_unknown='ignore'), ['artist']),\n",
    "        ('scale', StandardScaler(), ['duration_ms']),\n",
    "        ('pass', 'passthrough', ['explicit'])\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Ridge regression pipeline\n",
    "model = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('ridge', Ridge())\n",
    "])\n",
    "\n",
    "# Define the grid of alpha values to search\n",
    "param_grid = {\n",
    "    'ridge__alpha': np.logspace(-3, 2, 20)  # Alphas ranging from 0.001 to 100\n",
    "}\n",
    "\n",
    "# Perform Grid Search with cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    model,\n",
    "    param_grid,\n",
    "    scoring='neg_mean_squared_error',  # Use MSE as the scoring metric (negated)\n",
    "    cv=5,  # 5-fold cross-validation\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Best alpha and corresponding MSE\n",
    "best_alpha = grid_search.best_params_['ridge__alpha']\n",
    "best_mse = -grid_search.best_score_\n",
    "\n",
    "print(f\"Best alpha: {best_alpha}\")\n",
    "print(f\"Best cross-validated MSE: {best_mse}\")\n",
    "\n",
    "# Test set performance\n",
    "best_model = grid_search.best_estimator_\n",
    "y_test_pred = best_model.predict(X_test)\n",
    "test_mse = mean_squared_error(y_test, y_test_pred)\n",
    "test_rmse = np.sqrt(test_mse)\n",
    "print(f\"Test set MSE: {test_mse}\")\n",
    "print(f\"Test set RMSE: {test_rmse}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
